{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models often have many hyperparameters that need to be tuned to achieve maximal performance (e.g: learning rate, dropout rate, hidden layer dimension) . This motivates the need for hyperparameter tuners that intelligently search the space of hyperparameters that configure high performing model. \n",
    "\n",
    "To address this, MeTaL supports multiple hyperparameter tuners with an easy to use interface which allows users to streamline the hyperparameter optimization process. This tutorial covers utilizing MeTaL's hyperparameter tuners to tune an EndModel for maximal performance. Currently, two hyperparameter algorithms are supported:\n",
    "\n",
    "- <b>Random Search</b>\n",
    "- <b>Hyperband</b>\n",
    "\n",
    "The tutorial is broken down into the following sections \n",
    "\n",
    "1. <b>Setting up the Problem and Loading the Data</b>\n",
    "2. <b>Defining the Search Space</b>\n",
    "3. <b>Performing Random Search</b>\n",
    "4. <b>Performing Hyperband Search</b>\n",
    "5. <b>Comparing Random Search against Hyperband Search</b>\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before beginning, we first need to make sure that the metal/ directory is on our Python path. If the following cell runs without an error, you're all set. If not, make sure that you've installed snorkel-metal with pip or conda (or that you've added the repo to your path if you're running from source; for example, running source add_to_path.sh from the repository root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import metal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Problem and Loading the Data\n",
    "\n",
    "First let's set up our problem and load our data. For the purposes of this tutorial (and to keep the search process short) we use the small model we were introduced to in the basic tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic tutorial data\n",
    "from metal.utils import split_data\n",
    "import pickle\n",
    "\n",
    "with open(\"data/basics_tutorial.pkl\", 'rb') as f:\n",
    "    X, Y, L, D = pickle.load(f)\n",
    "    \n",
    "Xs, Ys, Ls, Ds = split_data(X, Y, L, D, splits=[0.8, 0.1, 0.1], stratify_by=Y, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's furthermore define and train our label model like we did in the basic tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[E:0]\tTrain Loss: 6.036\n",
      "[E:250]\tTrain Loss: 0.029\n",
      "[E:500]\tTrain Loss: 0.029\n",
      "[E:750]\tTrain Loss: 0.029\n",
      "[E:999]\tTrain Loss: 0.029\n",
      "Finished Training\n",
      "Accuracy: 0.879\n",
      "Precision: 0.771\n",
      "Recall: 0.724\n",
      "F1: 0.746\n",
      "Accuracy: 0.836\n",
      "Precision: 0.623\n",
      "Recall: 0.841\n",
      "F1: 0.716\n"
     ]
    }
   ],
   "source": [
    "# Train a the label model\n",
    "from metal.label_model import LabelModel\n",
    "label_model = LabelModel(k=2, seed=123)\n",
    "\n",
    "label_model.train(Ls[0], Y_dev=Ys[1], n_epochs=1000, print_every=250, lr=0.01, l2=1e-1)\n",
    "score = label_model.score(Ls[1], Ys[1])\n",
    "scores = label_model.score(Ls[1], Ys[1], metric=['precision', 'recall', 'f1'])\n",
    "\n",
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "scores = mv.score(Ls[1], Ys[1], metric=['accuracy', 'precision', 'recall', 'f1'])\n",
    "Y_train_ps = label_model.predict_proba(Ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our EndModel and verify that it successfully runs and achieves a decent score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Saving model at iteration 0 with best score 0.761\n",
      "[E:0]\tTrain Loss: 0.561\tDev score: 0.761\n",
      "Saving model at iteration 1 with best score 0.902\n",
      "[E:1]\tTrain Loss: 0.468\tDev score: 0.902\n",
      "[E:2]\tTrain Loss: 0.458\tDev score: 0.840\n",
      "[E:3]\tTrain Loss: 0.451\tDev score: 0.870\n",
      "[E:4]\tTrain Loss: 0.450\tDev score: 0.818\n",
      "Restoring best model from iteration 1 with score 0.902\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    202     0    \n",
      " l=2    44     754   \n"
     ]
    }
   ],
   "source": [
    "# Train an end model\n",
    "from metal.end_model import EndModel\n",
    "\n",
    "end_model_basic = EndModel([1000,10,2], \n",
    "                     batchnorm=True,\n",
    "                     dropout=.5,\n",
    "                     l2=.1,\n",
    "                     \n",
    "                     seed=123)\n",
    "\n",
    "end_model_basic.train(Xs[0], Y_train_ps, Xs[1], Ys[1], l2=0.1, batch_size=256, \n",
    "                n_epochs=5, print_every=1, validation_metric='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Notice that our F1 is around .902. In the sections below we will be trying to optimize the hyperparameters of this EndModel to achieve an even higher score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Search Space\n",
    "\n",
    "Before starting the hyperparameter tuning process, we need to specify the space of the hyperparameters we're searching. \n",
    "\n",
    "For the purposes of this tutorial we search over the following hyperparameters:\n",
    "- <b>n_epochs</b>: Integer representing the number of epochs to train\n",
    "- <b>batchnorm</b>: Boolean representing whether to use batch-normalization\n",
    "- <b>lr</b>: Float representing the learning rate for optimization\n",
    "- <b>layer_out_dims</b>: The architecture of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_epochs': [1, 5, 10],\n",
    "    'batchnorm' : [True, False],\n",
    "    'dropout': [0, .1, .2, .3, .4, .5],\n",
    "    'lr': {'range': [1e-5, 1], 'scale': 'log'},\n",
    "    'layer_out_dims' : [[1000,10,2], [1000, 100, 2]],\n",
    "    'print_every': 5,\n",
    "    'data_loader_config': [{\"batch_size\": 256, \"num_workers\": 1}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of what each line in the configuration means:\n",
    "\n",
    "- `'n_epochs': [1, 5, 10],`: This specifies that the hyperparameter tuner may train the model for either 1, 5 or 10 epochs\n",
    "- `'batchnorm' : [True, False],`: This specifies that a model instantiated by the tuner may have batchnorm as either True or False\n",
    "- `dropout': [0, .1, .2, .3, .4, .5]`: Like the above, this specifies that the dropout parameter of an instantiated model may be one of 0, .1, .2, .3, .4, or .5\n",
    "- `'lr': {'range': [1e-5, 1], 'scale': 'log'}`: This specifies that the learning rate of the training of a model may range from 1e-5 to 1, and that the tuner samples the learning rate on a log scale\n",
    "- `'layer_out_dims' : [[1000,10,2], [1000, 100, 2]]`: This specifies that upon instantiation of the model, the structure of the fully connected network can either be [1000, 10, 2] or [1000, 100, 2]; in the latter case, this means the network takes a 1000 dimensional input, has a hidden layer with 100 features and an output layer with 2 classes\n",
    "- `'print_every': 5`: This specifies that the model should print status updates every 5 iterations of training.\n",
    "- `'data_loader_config': [{\"batch_size\": 256, \"num_workers\": 1}],`: This specifies to use a batch of 256 for optimization\n",
    "\n",
    "Now that our search space is defined, let's start optimizing hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, random search has proven to be a powerful and efficient algorithm for tuning hyperparameters (see http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf for why). Let's use the RandomSearch tuner to find a good set of hyperparameters for our EndModel. Note that although we only do hyperparameter optimization for the EndModel, the tuners may also be used to do hyperparameter optimization for LabelModels.\n",
    "\n",
    "To start, let's import the RandomSearchTuner and instantiate our RandomSearchTuner to optimize an EndModel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.tuners.random_tuner import RandomSearchTuner\n",
    "rs_tuner = RandomSearchTuner(EndModel, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = [Xs[0], Y_train_ps]\n",
    "X_dev, Y_dev = Xs[1], Ys[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we're prepped to launch our random search! Performing the search is just as easy and requires just a single call to the `search` function.\n",
    "\n",
    "Most of the arguments to the `search` function below are self explanatory, but there are a couple of key arguments to watch out for:\n",
    "- `max_search` : This specifies the number of configurations to search over. As it is set to 10 below, this means we search over 10 random models and return the best one\n",
    "- `verbose`: This specifies whether the tuner should be verbose or not and can be used to turn on/off the its logging feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[1] Testing {'n_epochs': 10, 'batchnorm': False, 'dropout': 0, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.3700237151852522}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.754\n",
      "[E:0]\tTrain Loss: 0.994\tDev score: 0.754\n",
      "[E:5]\tTrain Loss: 0.577\tDev score: 0.754\n",
      "[E:9]\tTrain Loss: 0.577\tDev score: 0.754\n",
      "Restoring best model from iteration 0 with score 0.754\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     0      0    \n",
      " l=2    246    754   \n",
      "F1: 0.000\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[2] Testing {'n_epochs': 5, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 1.827334249624349e-05}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.742\n",
      "[E:0]\tTrain Loss: 0.676\tDev score: 0.742\n",
      "Saving model at iteration 1 with best score 0.753\n",
      "Saving model at iteration 2 with best score 0.755\n",
      "[E:4]\tTrain Loss: 0.624\tDev score: 0.754\n",
      "Restoring best model from iteration 2 with score 0.755\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     0      0    \n",
      " l=2    246    754   \n",
      "F1: 0.008\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[3] Testing {'n_epochs': 10, 'batchnorm': False, 'dropout': 0.4, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.6366544570068935}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.754\n",
      "[E:0]\tTrain Loss: 8.673\tDev score: 0.754\n",
      "Saving model at iteration 1 with best score 0.755\n",
      "[E:5]\tTrain Loss: 0.577\tDev score: 0.754\n",
      "[E:9]\tTrain Loss: 0.580\tDev score: 0.754\n",
      "Restoring best model from iteration 1 with score 0.755\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     1      0    \n",
      " l=2    245    754   \n",
      "F1: 0.008\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[4] Testing {'n_epochs': 5, 'batchnorm': True, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.31643979593790955}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.902\n",
      "[E:0]\tTrain Loss: 1.850\tDev score: 0.902\n",
      "Saving model at iteration 1 with best score 0.909\n",
      "Saving model at iteration 3 with best score 0.979\n",
      "[E:4]\tTrain Loss: 0.446\tDev score: 0.894\n",
      "Restoring best model from iteration 3 with score 0.979\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    236     6    \n",
      " l=2    10     748   \n",
      "F1: 0.957\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[5] Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.007292621889903268}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.976\n",
      "[E:0]\tTrain Loss: 0.486\tDev score: 0.976\n",
      "Restoring best model from iteration 0 with score 0.976\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    223     0    \n",
      " l=2    23     754   \n",
      "F1: 0.953\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[6] Testing {'n_epochs': 5, 'batchnorm': False, 'dropout': 0.4, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 1.670624993426239e-05}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.718\n",
      "[E:0]\tTrain Loss: 0.680\tDev score: 0.718\n",
      "Saving model at iteration 1 with best score 0.760\n",
      "[E:4]\tTrain Loss: 0.638\tDev score: 0.754\n",
      "Restoring best model from iteration 1 with score 0.760\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    14     22    \n",
      " l=2    232    732   \n",
      "F1: 0.140\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[7] Testing {'n_epochs': 5, 'batchnorm': False, 'dropout': 0.2, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.5013839780330771}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.754\n",
      "[E:0]\tTrain Loss: 0.972\tDev score: 0.754\n",
      "[E:4]\tTrain Loss: 0.578\tDev score: 0.754\n",
      "Restoring best model from iteration 0 with score 0.754\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     0      0    \n",
      " l=2    246    754   \n",
      "F1: 0.000\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[8] Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.5, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.008520966193723056}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.974\n",
      "[E:0]\tTrain Loss: 0.495\tDev score: 0.974\n",
      "Restoring best model from iteration 0 with score 0.974\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    221     0    \n",
      " l=2    25     754   \n",
      "F1: 0.953\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[9] Testing {'n_epochs': 10, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.003970906941573151}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.995\n",
      "[E:0]\tTrain Loss: 0.491\tDev score: 0.995\n",
      "[E:5]\tTrain Loss: 0.385\tDev score: 0.921\n",
      "[E:9]\tTrain Loss: 0.333\tDev score: 0.911\n",
      "Restoring best model from iteration 0 with score 0.995\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    244     1    \n",
      " l=2     2     753   \n",
      "F1: 0.994\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[10] Testing {'n_epochs': 1, 'batchnorm': True, 'dropout': 0.3, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0003777251862528499}\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at iteration 0 with best score 0.948\n",
      "[E:0]\tTrain Loss: 0.605\tDev score: 0.948\n",
      "Restoring best model from iteration 0 with score 0.948\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    243    58    \n",
      " l=2     3     696   \n",
      "F1: 0.909\n",
      "============================================================\n",
      "[SUMMARY]\n",
      "Best model: [9]\n",
      "Best config: {'n_epochs': 10, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'print_every': 5, 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.003970906941573151, 'seed': 131}\n",
      "Best score: 0.9938900203665988\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "best_rs_model = rs_tuner.search(search_space, X_dev, Y_dev, train_args=train_args, max_search=10, metric='f1', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our best random search model achieves an F1 of ~.994 which outperforms the model we had previously (F1 ~ .90). Can we do even better than random search by either attaining the same accuracy faster or achieving a higher score? The following section walks through using the <b>Hyperband</b> tuner, which recent research has shown to be more efficient than random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Hyperband Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While random search performs surprisingly well, we can be more efficient if we adaptively allocate more compute resources for configurations that perform well than to those that don't. For example if a configuration seems to yield a really poor model after the first epoch of training, it's unlikely it'll perform well even after more training, so we can early-terminate the training of this configuration to save compute. This is the core idea behind the <b>Hyperband</b> algorithm which recent research has shown to outperform various algorithms including random search. (See https://arxiv.org/abs/1603.06560 if interested!)\n",
    "\n",
    "Running Hyperband is just as easy as running random search. Let's import the HyperbandTuner and instantiate it. \n",
    "\n",
    "Note that there is one extra argument to initialize the HyperbandTuner:\n",
    "- `hyperband_epochs_budget`: This specifies the number of total epochs of training the tuner can perform in its search for a performant model. This is used to create the Hyperband search schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "|           Hyperband Schedule          |\n",
      "=========================================\n",
      "Table consists of tuples of (num configs, num_resources_per_config)which specify how many configs to run andfor how many epochs. \n",
      "Each bracket starts with a list of random configurations which is successively halved according the schedule.\n",
      "See the Hyperband paper (https://arxiv.org/pdf/1603.06560.pdf) for more details.\n",
      "-----------------------------------------\n",
      "Bracket 0: (9, 1) (3, 4) (1, 13)\n",
      "Bracket 1: (3, 4) (1, 13)\n",
      "Bracket 2: (3, 13)\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from metal.tuners.hyperband_tuner import HyperbandTuner\n",
    "hb_tuner = HyperbandTuner(EndModel, hyperband_epochs_budget=100, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch the Hyperband search process using the same `search` call. Note that since the Hyperband schedule already limits the amount of compute we do, we don't have to set the `max_search` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[0 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.3700237151852522}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.754\n",
      "[E:0]\tTrain Loss: 0.994\tDev score: 0.754\n",
      "Restoring best model from iteration 0 with score 0.754\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     0      0    \n",
      " l=2    246    754   \n",
      "F1: 0.000\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[1 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.019515100267567337}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.979\n",
      "[E:0]\tTrain Loss: 0.499\tDev score: 0.979\n",
      "Restoring best model from iteration 0 with score 0.979\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    225     0    \n",
      " l=2    21     754   \n",
      "F1: 0.955\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[2 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.4, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 7.089807415516936e-05}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.766\n",
      "[E:0]\tTrain Loss: 0.696\tDev score: 0.766\n",
      "Restoring best model from iteration 0 with score 0.766\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    72     72    \n",
      " l=2    174    682   \n",
      "F1: 0.402\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[3 Testing {'n_epochs': 1, 'batchnorm': True, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0004837052086066461}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.965\n",
      "[E:0]\tTrain Loss: 0.593\tDev score: 0.965\n",
      "Restoring best model from iteration 0 with score 0.965\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    245    33    \n",
      " l=2     1     721   \n",
      "F1: 0.946\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[4 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.000903579845523744}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.937\n",
      "[E:0]\tTrain Loss: 0.537\tDev score: 0.937\n",
      "Restoring best model from iteration 0 with score 0.937\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    182     0    \n",
      " l=2    64     754   \n",
      "F1: 0.866\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[5 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.4, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 2.3251656081342196e-05}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.744\n",
      "[E:0]\tTrain Loss: 0.678\tDev score: 0.744\n",
      "Restoring best model from iteration 0 with score 0.744\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    32     52    \n",
      " l=2    214    702   \n",
      "F1: 0.179\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[6 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.2, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0019223627877981123}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.798\n",
      "[E:0]\tTrain Loss: 0.573\tDev score: 0.798\n",
      "Restoring best model from iteration 0 with score 0.798\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    37      0    \n",
      " l=2    209    754   \n",
      "F1: 0.298\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[7 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.5, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.007342476624077729}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.972\n",
      "[E:0]\tTrain Loss: 0.495\tDev score: 0.972\n",
      "Restoring best model from iteration 0 with score 0.972\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    218     1    \n",
      " l=2    28     753   \n",
      "F1: 0.953\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[8 Testing {'n_epochs': 1, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.00047921510264231735}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.757\n",
      "[E:0]\tTrain Loss: 0.562\tDev score: 0.757\n",
      "Restoring best model from iteration 0 with score 0.757\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     3      0    \n",
      " l=2    243    754   \n",
      "F1: 0.024\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[9 Testing {'n_epochs': 4, 'batchnorm': False, 'dropout': 0.1, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.019515100267567337}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.979\n",
      "[E:0]\tTrain Loss: 0.499\tDev score: 0.979\n",
      "[E:3]\tTrain Loss: 0.420\tDev score: 0.962\n",
      "Restoring best model from iteration 0 with score 0.979\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    226     0    \n",
      " l=2    20     754   \n",
      "F1: 0.953\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[10 Testing {'n_epochs': 4, 'batchnorm': False, 'dropout': 0.5, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.007342476624077729}\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at iteration 0 with best score 0.972\n",
      "[E:0]\tTrain Loss: 0.495\tDev score: 0.972\n",
      "[E:3]\tTrain Loss: 0.437\tDev score: 0.915\n",
      "Restoring best model from iteration 0 with score 0.972\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    221     1    \n",
      " l=2    25     753   \n",
      "F1: 0.946\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[11 Testing {'n_epochs': 4, 'batchnorm': True, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0004837052086066461}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.965\n",
      "[E:0]\tTrain Loss: 0.593\tDev score: 0.965\n",
      "Saving model at iteration 1 with best score 0.975\n",
      "[E:3]\tTrain Loss: 0.431\tDev score: 0.954\n",
      "Restoring best model from iteration 1 with score 0.975\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    242    12    \n",
      " l=2     4     742   \n",
      "F1: 0.960\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[12 Testing {'n_epochs': 13, 'batchnorm': True, 'dropout': 0.2, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0004837052086066461}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.965\n",
      "[E:0]\tTrain Loss: 0.593\tDev score: 0.965\n",
      "Saving model at iteration 1 with best score 0.975\n",
      "[E:5]\tTrain Loss: 0.361\tDev score: 0.936\n",
      "[E:10]\tTrain Loss: 0.322\tDev score: 0.931\n",
      "[E:12]\tTrain Loss: 0.319\tDev score: 0.918\n",
      "Restoring best model from iteration 1 with score 0.975\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    241    19    \n",
      " l=2     5     735   \n",
      "F1: 0.968\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[13 Testing {'n_epochs': 4, 'batchnorm': True, 'dropout': 0.3, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.000611420954333822}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.944\n",
      "[E:0]\tTrain Loss: 0.636\tDev score: 0.944\n",
      "Saving model at iteration 1 with best score 0.970\n",
      "[E:3]\tTrain Loss: 0.423\tDev score: 0.944\n",
      "Restoring best model from iteration 1 with score 0.970\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    244    23    \n",
      " l=2     2     731   \n",
      "F1: 0.944\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[14 Testing {'n_epochs': 4, 'batchnorm': False, 'dropout': 0.2, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.03504434356854102}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.962\n",
      "[E:0]\tTrain Loss: 0.506\tDev score: 0.962\n",
      "Saving model at iteration 1 with best score 0.982\n",
      "[E:3]\tTrain Loss: 0.473\tDev score: 0.973\n",
      "Restoring best model from iteration 1 with score 0.982\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    237    10    \n",
      " l=2     9     744   \n",
      "F1: 0.948\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[15 Testing {'n_epochs': 4, 'batchnorm': True, 'dropout': 0.4, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0028206265770237622}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.989\n",
      "[E:0]\tTrain Loss: 0.617\tDev score: 0.989\n",
      "[E:3]\tTrain Loss: 0.444\tDev score: 0.928\n",
      "Restoring best model from iteration 0 with score 0.989\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    246    12    \n",
      " l=2     0     742   \n",
      "F1: 0.978\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4)\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[16 Testing {'n_epochs': 13, 'batchnorm': True, 'dropout': 0.4, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0028206265770237622}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.989\n",
      "[E:0]\tTrain Loss: 0.617\tDev score: 0.989\n",
      "[E:5]\tTrain Loss: 0.434\tDev score: 0.928\n",
      "[E:10]\tTrain Loss: 0.419\tDev score: 0.920\n",
      "[E:12]\tTrain Loss: 0.411\tDev score: 0.911\n",
      "Restoring best model from iteration 0 with score 0.989\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    246     5    \n",
      " l=2     0     749   \n",
      "F1: 0.970\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[17 Testing {'n_epochs': 13, 'batchnorm': True, 'dropout': 0.5, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 6.358854167922539e-05}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.610\n",
      "[E:0]\tTrain Loss: 0.760\tDev score: 0.610\n",
      "Saving model at iteration 1 with best score 0.680\n",
      "Saving model at iteration 2 with best score 0.722\n",
      "Saving model at iteration 3 with best score 0.735\n",
      "Saving model at iteration 4 with best score 0.796\n",
      "Saving model at iteration 5 with best score 0.832\n",
      "[E:5]\tTrain Loss: 0.628\tDev score: 0.832\n",
      "Saving model at iteration 6 with best score 0.849\n",
      "Saving model at iteration 7 with best score 0.860\n",
      "Saving model at iteration 9 with best score 0.895\n",
      "[E:10]\tTrain Loss: 0.586\tDev score: 0.874\n",
      "Saving model at iteration 11 with best score 0.899\n",
      "Saving model at iteration 12 with best score 0.915\n",
      "[E:12]\tTrain Loss: 0.568\tDev score: 0.915\n",
      "Restoring best model from iteration 12 with score 0.915\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    243    100   \n",
      " l=2     3     654   \n",
      "F1: 0.830\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[18 Testing {'n_epochs': 13, 'batchnorm': False, 'dropout': 0, 'layer_out_dims': [1000, 10, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 2.5548270372671158e-05}\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at iteration 0 with best score 0.258\n",
      "[E:0]\tTrain Loss: 0.739\tDev score: 0.258\n",
      "Saving model at iteration 1 with best score 0.296\n",
      "Saving model at iteration 2 with best score 0.377\n",
      "Saving model at iteration 3 with best score 0.498\n",
      "Saving model at iteration 4 with best score 0.649\n",
      "Saving model at iteration 5 with best score 0.730\n",
      "[E:5]\tTrain Loss: 0.674\tDev score: 0.730\n",
      "Saving model at iteration 6 with best score 0.778\n",
      "Saving model at iteration 7 with best score 0.783\n",
      "[E:10]\tTrain Loss: 0.612\tDev score: 0.772\n",
      "[E:12]\tTrain Loss: 0.594\tDev score: 0.765\n",
      "Restoring best model from iteration 7 with score 0.783\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1    48     20    \n",
      " l=2    198    734   \n",
      "F1: 0.306\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): IdentityModule()\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (2): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "[19 Testing {'n_epochs': 13, 'batchnorm': False, 'dropout': 0.5, 'layer_out_dims': [1000, 100, 2], 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.3190709818793598}\n",
      "============================================================\n",
      "Saving model at iteration 0 with best score 0.754\n",
      "[E:0]\tTrain Loss: 3.401\tDev score: 0.754\n",
      "Saving model at iteration 1 with best score 0.755\n",
      "[E:5]\tTrain Loss: 0.589\tDev score: 0.754\n",
      "[E:10]\tTrain Loss: 0.576\tDev score: 0.754\n",
      "[E:12]\tTrain Loss: 0.576\tDev score: 0.754\n",
      "Restoring best model from iteration 1 with score 0.755\n",
      "Finished Training\n",
      "Confusion Matrix (Dev)\n",
      "        y=1    y=2   \n",
      " l=1     1      0    \n",
      " l=2    245    754   \n",
      "F1: 0.008\n",
      "============================================================\n",
      "[SUMMARY]\n",
      "Best model: [15]\n",
      "Best config: {'n_epochs': 13, 'batchnorm': True, 'dropout': 0.4, 'layer_out_dims': [1000, 10, 2], 'print_every': 5, 'data_loader_config': {'batch_size': 256, 'num_workers': 1}, 'lr': 0.0028206265770237622, 'seed': 138}\n",
      "Best score: 0.9778672032193159\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EndModel(\n",
       "  (network): Sequential(\n",
       "    (0): IdentityModule()\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=10, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.4)\n",
       "    )\n",
       "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  )\n",
       "  (criteria): SoftCrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hb_model = hb_tuner.search(search_space, X_dev, Y_dev, train_args=train_args, metric='f1', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we achieved an F1 ~.97, which beat our initial F1 ~.90. However, unfortunately it did not beat the score achieved by random search score. The next section will compare the performances of random search and hyperband using the logged data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Random Search against Hyperband Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
