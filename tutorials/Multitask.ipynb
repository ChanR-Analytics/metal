{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Supervision Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we demonstrate how to use the multi-task versions of the label model and end model. We do this with a simple synthetic dataset, focusing primarily on inputs/output interfaces of these models. In a future tutorial, we will demonstrate the multi-task workflow on a real-world domain of larger scale and complexity, and the benefits that come from jointly modeling the weak supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-task problems, we execute our pipeline in five steps:\n",
    "1. Load Data\n",
    "2. Define Task Graph\n",
    "3. Train Label Model\n",
    "4. Train End Model\n",
    "5. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load our data.\n",
    "\n",
    "The data dyptes for the multi-task setting mirror those of the single-task setting, but with an extra dimension for the number of tasks (t), and with the cardinality (k) being replaced by task-specific cardinalities (K_t):\n",
    "\n",
    "* X: a t-length list of \\[n\\]-dim iterables of end model inputs OR a single \\[n\\]-dim iterable of inputs if all tasks operate on the same input.\n",
    "* Y: a t-length list of \\[n\\]-dim numpy.ndarray of target labels (Y[i] $\\in$ {1,...,K_t})\n",
    "* L: a t-length list of \\[n,m\\] scipy.sparse matrices of noisy labels (L[i,j] $\\in$ {0,...,K_t}, with label 0 reserved for abstentions\n",
    "\n",
    "And optionally (for use with some debugging/analysis tools):\n",
    "* D: a t-length list of \\[n\\]-dim iterables of human-readable examples OR a single \\[n\\]-dim iterable of examples if all tasks operate on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/multitask_tutorial.pkl\", 'rb') as f:\n",
    "    X, Y, L, D = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Task Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary role of the task graph is to define a set of feasible target label vectors.\n",
    "For example, consider the following set of classification tasks, wherein we assign text entities to one of the given labels:\n",
    "\n",
    "T0: Y0 ∈ {PERSON, ORG}  \n",
    "T1: Y1 ∈ {DOCTOR, OTHER PERSON, NOT APPLICABLE}  \n",
    "T2: Y2 ∈ {HOSPITAL, OTHER ORG, NOT APPLICABLE}  \n",
    "\n",
    "Observe that the tasks are related by logical implication relationships: if Y0 = PERSON,\n",
    "then Y2 = NOT APPLICABLE, since Y2 classifies ORGs. Thus, in this task structure, [PERSON, DOCTOR, NOT APPLICABLE] is a feasible label vector, whereas [PERSON, DOCTOR, HOSPITAL] is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reflect this feasible label set, we define our task graph for this problem with a TaskHierarchy, a subclass of TaskGraph which assumes that label K_t for each non-root node is the \"NOT APPLICABLE\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.multitask import TaskHierarchy\n",
    "task_graph = TaskHierarchy(cardinalities=[2,3,3], edges=[(0,1), (0,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass our TaskGraph into the multi-task label model to instantiate a model with the appropriate structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.multitask import MTLabelModel\n",
    "label_model = MTLabelModel(task_graph=task_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[E:0]\tTrain Loss: 3.967\n",
      "[E:10]\tTrain Loss: 1.118\n",
      "[E:20]\tTrain Loss: 0.587\n",
      "[E:30]\tTrain Loss: 0.186\n",
      "[E:40]\tTrain Loss: 0.081\n",
      "[E:50]\tTrain Loss: 0.049\n",
      "[E:60]\tTrain Loss: 0.026\n",
      "[E:70]\tTrain Loss: 0.026\n",
      "[E:80]\tTrain Loss: 0.024\n",
      "[E:90]\tTrain Loss: 0.023\n",
      "[E:99]\tTrain Loss: 0.023\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "label_model.train(L, n_epochs=100, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the single-task case, we can score this trained model to evaluate it directly, or use it to make predictions for our training set that will then be used to train a multi-task end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9390000000000001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_model.score(L, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_ps stands for \"Y[labels]_train[split]_p[redicted]s[oft]\"\n",
    "Y_train_ps = label_model.predict_proba(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the single-task end model, the multi-task end model consists of three components: input layers, middle layers, and task head layers. Again, each layer consists of a torch.nn.Module followed by various optional additional operators (e.g., a ReLU nonlinearity, batch normalization, and/or dropout).\n",
    "\n",
    "**Input layers**: The input module is an IdentityModule by default. If your tasks accept inputs of different types (e.g., one task over images and another over text), you may pass in a t-length list of input modules.\n",
    "\n",
    "**Middle layers**: The middle modules are nn.Linear by default and are shared by all tasks.\n",
    "\n",
    "**Head layers**: The t task head modules are nn.Linear modules by default. You may instead pass in a custom module to be used by all tasks or a t-length list of modules. These task heads are unique to each task, sharing no parameters with other tasks. Their output is fed to a softmax operators whose output dimensions are equal to the cardinalties for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a simple graph with a single (identity) input module, two intermediate layers, and linear task heads attached to the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network architecture:\n",
      "\n",
      "--Input Layer--\n",
      "IdentityModule()\n",
      "\n",
      "--Middle Layers--\n",
      "(layer1):\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "(layer2):\n",
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "(head0)\n",
      "Linear(in_features=10, out_features=2, bias=True)\n",
      "(head1)\n",
      "Linear(in_features=10, out_features=3, bias=True)\n",
      "(head2)\n",
      "Linear(in_features=10, out_features=3, bias=True)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metal.multitask import MTEndModel\n",
    "\n",
    "end_model = MTEndModel([1000,100,10], task_graph=task_graph, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at iteration 0 with best score 0.946\n",
      "[E:0]\tTrain Loss: 1.967\tDev score: 0.946\n",
      "Saving model at iteration 1 with best score 0.968\n",
      "[E:1]\tTrain Loss: 0.866\tDev score: 0.968\n",
      "[E:2]\tTrain Loss: 0.614\tDev score: 0.956\n",
      "[E:3]\tTrain Loss: 0.548\tDev score: 0.957\n",
      "[E:4]\tTrain Loss: 0.501\tDev score: 0.945\n",
      "[E:5]\tTrain Loss: 0.486\tDev score: 0.953\n",
      "[E:6]\tTrain Loss: 0.471\tDev score: 0.948\n",
      "[E:7]\tTrain Loss: 0.447\tDev score: 0.948\n",
      "[E:8]\tTrain Loss: 0.431\tDev score: 0.949\n",
      "[E:9]\tTrain Loss: 0.425\tDev score: 0.949\n",
      "Restoring best model from iteration 1 with score 0.968\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "end_model.train(X, Y_train_ps, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes scoring our multi-task models, by default the mean task accuracy is reported. We can also, however, pass `reduce=None` to get back a list of task-specific accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model:\n",
      "Accuracy: 0.939\n",
      "\n",
      "End Model:\n",
      "Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "print(\"Label Model:\")\n",
    "score = label_model.score(L, Y)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"End Model:\")\n",
    "score = end_model.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
