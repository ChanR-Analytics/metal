{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "MAX_SEQ_LEN = 10\n",
    "MAX_INT = 8\n",
    "\n",
    "# Memorize first\n",
    "# X = torch.randint(1, MAX_INT + 1, (N,MAX_SEQ_LEN)).long()\n",
    "# Y = X[:,0]\n",
    "# vocab_size = MAX_INT + 1\n",
    "\n",
    "# Notice pre-marker\n",
    "X = []\n",
    "for i in range(N):\n",
    "    seq_len = int(torch.randint(3, MAX_SEQ_LEN + 1, (1,)))\n",
    "    X.append(torch.randint(1, MAX_INT + 1, (seq_len,)).long())\n",
    "Y = torch.zeros(N).long()\n",
    "for i in range(N):\n",
    "    needle = np.random.randint(2, len(X[i]))\n",
    "    X[i][needle] = MAX_INT + 1\n",
    "    Y[i] = X[i][needle - 1]\n",
    "vocab_size = MAX_INT + 2\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "X2 = sorted(X, key=lambda x:-len(x))\n",
    "X3 = pad_sequence(X2, batch_first=True)\n",
    "\n",
    "# input_size = 4\n",
    "# hidden_size = 10\n",
    "# bidirectional = False\n",
    "\n",
    "# Xs = [X[:800], X[800:900], X[900:]]\n",
    "# Ys = [Y[:800], Y[800:900], Y[900:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  3,  1,  ...,  7,  9,  8],\n",
       "        [ 1,  8,  4,  ...,  1,  6,  8],\n",
       "        [ 3,  3,  5,  ...,  7,  9,  2],\n",
       "        ...,\n",
       "        [ 2,  5,  9,  ...,  0,  0,  0],\n",
       "        [ 5,  3,  9,  ...,  0,  0,  0],\n",
       "        [ 1,  7,  9,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  8,  7,  9,  4,  3,  5,  1,  5],\n",
       "        [ 5,  8,  3,  9,  0,  0,  0,  0,  0,  0],\n",
       "        [ 6,  6,  9,  1,  8,  2,  3,  8,  0,  0],\n",
       "        [ 1,  8,  9,  6,  3,  0,  0,  0,  0,  0],\n",
       "        [ 2,  3,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 2,  7,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5,  6,  7,  6,  9,  2,  7,  0,  0,  0],\n",
       "        [ 1,  4,  1,  7,  8,  1,  9,  0,  0,  0],\n",
       "        [ 5,  6,  2,  6,  2,  9,  7,  1,  1,  4],\n",
       "        [ 6,  2,  8,  4,  9,  1,  8,  4,  4,  0],\n",
       "        [ 3,  5,  1,  9,  5,  5,  0,  0,  0,  0],\n",
       "        [ 6,  5,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  5,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 4,  6,  9,  7,  2,  1,  0,  0,  0,  0],\n",
       "        [ 2,  7,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5,  6,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 7,  1,  7,  9,  5,  7,  5,  7,  0,  0],\n",
       "        [ 6,  6,  9,  1,  5,  3,  6,  4,  0,  0],\n",
       "        [ 4,  6,  2,  2,  1,  2,  1,  9,  1,  0],\n",
       "        [ 1,  7,  9,  6,  0,  0,  0,  0,  0,  0],\n",
       "        [ 6,  5,  8,  5,  8,  2,  3,  3,  9,  6],\n",
       "        [ 4,  4,  9,  6,  7,  4,  5,  0,  0,  0],\n",
       "        [ 1,  2,  8,  7,  9,  4,  3,  5,  1,  5],\n",
       "        [ 8,  8,  9,  8,  3,  1,  1,  8,  7,  8],\n",
       "        [ 8,  5,  6,  9,  4,  4,  0,  0,  0,  0],\n",
       "        [ 8,  8,  1,  1,  9,  6,  1,  4,  8,  0],\n",
       "        [ 2,  7,  9,  7,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5,  6,  2,  8,  1,  6,  7,  9,  6,  2],\n",
       "        [ 2,  6,  6,  9,  5,  0,  0,  0,  0,  0],\n",
       "        [ 5,  1,  4,  9,  6,  7,  0,  0,  0,  0],\n",
       "        [ 2,  3,  6,  3,  6,  2,  9,  3,  2,  4],\n",
       "        [ 4,  5,  9,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X3[np.random.randint(0,1000,32),:]\n",
    "X.shape\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_seq = X.shape\n",
    "seq_lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "for i in range(batch_size):\n",
    "    for j in range(max_seq - 1, -1, -1):\n",
    "        if X[i, j] != 0:\n",
    "            seq_lengths[i] = j + 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,   4,   8,   5,   3,   3,   7,   7,  10,   9,   6,   3,\n",
       "          3,   6,   3,   3,   8,   8,   9,   4,  10,   7,  10,  10,\n",
       "          6,   9,   4,  10,   5,   6,  10,   3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  8,  5,  4,  4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ys[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: LSTM currently assumes sequences end at the first 0.\n",
      "Using randomly initialized embeddings.\n",
      "Embeddings shape = (10, 4)\n",
      "The embeddings are NOT FROZEN\n",
      "Using lstm_reduction = 'max'\n",
      "Overwriting layer_output_dims=[100, 50] to layer_output_dims=[10, 8]\n",
      "\n",
      "Network architecture:\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): LSTMModule(\n",
      "      (embeddings): Embedding(10, 4)\n",
      "      (lstm): LSTM(4, 10, batch_first=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): Linear(in_features=8, out_features=8, bias=True)\n",
      ")\n",
      "\n",
      "Reaffirming n_epochs=20\n",
      "Reaffirming print_every=5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 115, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 10 in dimension 1 at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:3586\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2111b9038967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0minput_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               layer_output_dims=[hidden_size * (bidirectional + 1), MAX_INT])\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/metal/metal/end_model/end_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_dev, Y_dev, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/bradenjh/anaconda3/envs/metal/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 115, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 10 in dimension 1 at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:3586\n"
     ]
    }
   ],
   "source": [
    "from metal.end_model import EndModel\n",
    "from metal.input_modules import LSTMModule\n",
    "\n",
    "\n",
    "\n",
    "lstm_module = LSTMModule(input_size, hidden_size, vocab_size, bidirectional=bidirectional)\n",
    "em = EndModel(cardinality=MAX_INT, \n",
    "              input_module=lstm_module, \n",
    "              layer_output_dims=[hidden_size * (bidirectional + 1), MAX_INT])\n",
    "em.train(Xs[0], Ys[0], Xs[1], Ys[1], n_epochs=20, print_every=5)\n",
    "em.score(Xs[2], Ys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metal]",
   "language": "python",
   "name": "conda-env-metal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
